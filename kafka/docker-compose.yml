version: "3.7"

services:
  # -- Kafka & Dependencies -- ##

  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.0
    networks:
      - protect
    ports:
      - 2181:2181
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      KAFKA_OPTS: '-Djava.security.auth.login.config=/etc/kafka/server-jaas.conf -Dzookeeper.authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider'
    volumes:
      - ../kafka/config/server-jaas.conf:/etc/kafka/server-jaas.conf:ro,z

  kafka:
    image: confluentinc/cp-enterprise-kafka:5.5.0
    depends_on:
      - zookeeper
    networks:
      - protect
    ports:
      - 9092:9092
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      # If you don't hardcode a broker ID, a new one will be created for you after every container restart.
      # But this would be bad, because any topics you had already created will have been mapped to the previous
      # broker ID (now gone and never to be used again).  Thus, those topics will be without any broker, and your
      # attempts to create a new message in them will result in a "leader [broker] is unavailable" error.
      KAFKA_BROKER_ID: '0'

      # INTERNAL - Exposed to Docker's internal network, used by automated testing env
      # EXTERNAL - Exposed to outside of the Docker network (localhost), used for local dev
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,INTERNAL_SSL://kafka:29093,EXTERNAL_SSL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,INTERNAL_SSL:SASL_SSL,EXTERNAL_SSL:SASL_SSL
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: '0'

      # support for SASL_SSL, so the way we auth to our localhost Kafka cluster will match what we do in production
      KAFKA_SSL_KEYSTORE_FILENAME: 'kafka.server.keystore.jks'
      KAFKA_SSL_KEYSTORE_CREDENTIALS: 'keystore_creds'
      KAFKA_SSL_KEY_CREDENTIALS: 'sslkey_creds'
      KAFKA_SSL_TRUSTSTORE_FILENAME: 'kafka.server.truststore.jks'
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: 'truststore_creds'
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: 'PLAIN'
      KAFKA_SASL_ENABLED_MECHANISMS: 'PLAIN,SCRAM-SHA-256,SCRAM-SHA-512'
      KAFKA_OPTS: '-Djava.security.auth.login.config=/opt/kafka/config/server-jaas.conf'

      # log level overrides, see https://github.com/confluentinc/cp-docker-images/blob/master/debian/kafka/include/etc/confluent/docker/log4j.properties.template
      KAFKA_LOG4J_LOGGERS: 'kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO'

      CONFLUENT_SUPPORT_METRICS_ENABLE: 'false'
    volumes:
      - ../kafka/config/kafka.server.keystore.jks:/etc/kafka/secrets/kafka.server.keystore.jks:ro,z
      - ../kafka/config/kafka.server.truststore.jks:/etc/kafka/secrets/kafka.server.truststore.jks:ro,z
      - ../kafka/config/keystore_creds:/etc/kafka/secrets/keystore_creds:ro,z
      - ../kafka/config/sslkey_creds:/etc/kafka/secrets/sslkey_creds:ro,z
      - ../kafka/config/truststore_creds:/etc/kafka/secrets/truststore_creds:ro,z
      - ../kafka/config/server-jaas.conf:/opt/kafka/config/server-jaas.conf:ro,z

  kafka-ui:
    image: obsidiandynamics/kafdrop
    depends_on:
      - kafka
    networks:
      - protect
    ports:
      - '9000:9000'
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
      JVM_OPTS: -Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify
